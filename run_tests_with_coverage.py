#!/usr/bin/env python3
"""
Comprehensive test runner with code coverage measurement
Runs all tests and generates detailed coverage reports
"""

import os
import sys
import subprocess
import json
from datetime import datetime
from pathlib import Path

def run_command(command, description=""):
    """Run a shell command and return success status"""
    print(f"ðŸ”„ {description}")
    print(f"   Command: {command}")
    
    try:
        result = subprocess.run(command, shell=True, capture_output=True, text=True, timeout=300)
        
        if result.returncode == 0:
            print(f"   âœ… Success")
            if result.stdout.strip():
                print(f"   Output: {result.stdout.strip()}")
            return True
        else:
            print(f"   âŒ Failed (exit code: {result.returncode})")
            if result.stderr.strip():
                print(f"   Error: {result.stderr.strip()}")
            return False
            
    except subprocess.TimeoutExpired:
        print(f"   âŒ Timeout (5 minutes)")
        return False
    except Exception as e:
        print(f"   âŒ Exception: {e}")
        return False

def check_test_files():
    """Check what test files are available"""
    test_files = []
    
    # Look for test files
    for pattern in ["test_*.py", "*_test.py", "tests/*.py"]:
        for file_path in Path(".").glob(pattern):
            if file_path.is_file():
                test_files.append(str(file_path))
    
    # Look in tests directory
    tests_dir = Path("tests")
    if tests_dir.exists():
        for file_path in tests_dir.glob("*.py"):
            if file_path.name != "__init__.py":
                test_files.append(str(file_path))
    
    return test_files

def run_integration_tests():
    """Run integration tests with coverage"""
    print("ðŸ§ª Running Integration Tests with Coverage")
    print("=" * 50)
    
    # Check if integration tests exist
    integration_test = "tests/integration_tests.py"
    if not os.path.exists(integration_test):
        print(f"âŒ Integration test file not found: {integration_test}")
        return False
    
    # Run integration tests with coverage
    success = run_command(
        f"coverage run --append {integration_test}",
        "Running integration tests with coverage measurement"
    )
    
    return success

def run_unit_tests():
    """Run unit tests with pytest and coverage"""
    print("\nðŸ”¬ Running Unit Tests with Coverage")
    print("=" * 50)
    
    test_files = check_test_files()
    unit_test_files = [f for f in test_files if "integration" not in f]
    
    if not unit_test_files:
        print("â„¹ï¸  No unit test files found, creating a basic test")
        
        # Create a basic test file if none exist
        basic_test_content = '''
import unittest
import os
import json

class TestBasicFunctionality(unittest.TestCase):
    """Basic functionality tests"""
    
    def test_encyclopedia_file_exists(self):
        """Test that encyclopedia.json exists and is valid"""
        self.assertTrue(os.path.exists("encyclopedia.json"))
        
        with open("encyclopedia.json", "r") as f:
            data = json.load(f)
            self.assertIsInstance(data, list)
            self.assertGreater(len(data), 0)
    
    def test_docs_directory_exists(self):
        """Test that docs directory exists"""
        self.assertTrue(os.path.exists("docs"))
        self.assertTrue(os.path.isdir("docs"))
    
    def test_examples_directory_exists(self):
        """Test that examples directory exists"""
        self.assertTrue(os.path.exists("examples"))
        self.assertTrue(os.path.isdir("examples"))

if __name__ == "__main__":
    unittest.main()
'''
        
        with open("tests/test_basic.py", "w") as f:
            f.write(basic_test_content)
        
        unit_test_files = ["tests/test_basic.py"]
    
    # Run unit tests with coverage
    for test_file in unit_test_files:
        if "integration" not in test_file:
            success = run_command(
                f"coverage run --append {test_file}",
                f"Running unit tests: {test_file}"
            )
            if not success:
                print(f"âš ï¸  Unit test failed: {test_file}")
    
    return True

def generate_coverage_reports():
    """Generate comprehensive coverage reports"""
    print("\nðŸ“Š Generating Coverage Reports")
    print("=" * 50)
    
    # Generate console report
    run_command("coverage report", "Generating console coverage report")
    
    # Generate HTML report
    run_command("coverage html", "Generating HTML coverage report")
    
    # Generate XML report (for CI/CD)
    run_command("coverage xml", "Generating XML coverage report")
    
    # Generate JSON report for programmatic access
    run_command("coverage json", "Generating JSON coverage report")
    
    return True

def analyze_coverage_results():
    """Analyze coverage results and provide recommendations"""
    print("\nðŸ” Analyzing Coverage Results")
    print("=" * 50)
    
    try:
        # Read JSON coverage report
        if os.path.exists("coverage.json"):
            with open("coverage.json", "r") as f:
                coverage_data = json.load(f)
            
            total_coverage = coverage_data["totals"]["percent_covered"]
            
            print(f"ðŸ“ˆ Overall Coverage: {total_coverage:.2f}%")
            
            # Analyze by file
            files_coverage = []
            for filename, file_data in coverage_data["files"].items():
                if not any(skip in filename for skip in ["test_", "/tests/", "__pycache__"]):
                    files_coverage.append({
                        "file": filename,
                        "coverage": file_data["summary"]["percent_covered"]
                    })
            
            # Sort by coverage (lowest first)
            files_coverage.sort(key=lambda x: x["coverage"])
            
            print(f"\nðŸ“‹ Files needing attention (lowest coverage):")
            for file_info in files_coverage[:5]:
                print(f"   {file_info['file']}: {file_info['coverage']:.1f}%")
            
            # Coverage quality assessment
            if total_coverage >= 90:
                grade = "A+ (Excellent)"
                recommendation = "Maintain current coverage levels"
            elif total_coverage >= 80:
                grade = "A (Very Good)"
                recommendation = "Consider increasing coverage for critical files"
            elif total_coverage >= 70:
                grade = "B (Good)"
                recommendation = "Focus on testing core functionality"
            elif total_coverage >= 60:
                grade = "C (Acceptable)"
                recommendation = "Significant testing improvements needed"
            else:
                grade = "F (Poor)"
                recommendation = "Critical: Implement comprehensive testing"
            
            print(f"\nðŸŽ¯ Coverage Grade: {grade}")
            print(f"ðŸ’¡ Recommendation: {recommendation}")
            
            return {
                "total_coverage": total_coverage,
                "grade": grade,
                "files_analyzed": len(files_coverage),
                "recommendation": recommendation
            }
            
    except Exception as e:
        print(f"âŒ Error analyzing coverage: {e}")
        return None

def create_coverage_summary():
    """Create a summary report of all testing activities"""
    print("\nðŸ“ Creating Coverage Summary")
    print("=" * 50)
    
    summary = {
        "timestamp": datetime.now().isoformat(),
        "test_run_summary": {
            "integration_tests": os.path.exists("integration_test_report_*.json"),
            "unit_tests": True,
            "coverage_measurement": True
        },
        "reports_generated": {
            "html_report": os.path.exists("coverage_html_report/index.html"),
            "xml_report": os.path.exists("coverage.xml"),
            "json_report": os.path.exists("coverage.json")
        }
    }
    
    # Add coverage analysis if available
    coverage_analysis = analyze_coverage_results()
    if coverage_analysis:
        summary["coverage_analysis"] = coverage_analysis
    
    # Save summary
    with open("test_coverage_summary.json", "w") as f:
        json.dump(summary, f, indent=2)
    
    print(f"ðŸ’¾ Summary saved to: test_coverage_summary.json")
    
    return summary

def main():
    """Main test runner function"""
    print("ðŸš€ Comprehensive Test Runner with Coverage")
    print("=" * 60)
    print(f"ðŸ“… Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # Initialize coverage
    run_command("coverage erase", "Clearing previous coverage data")
    
    # Run tests
    integration_success = run_integration_tests()
    unit_success = run_unit_tests()
    
    # Generate reports
    generate_coverage_reports()
    
    # Analyze results
    summary = create_coverage_summary()
    
    # Final summary
    print("\n" + "=" * 60)
    print("ðŸ“Š FINAL SUMMARY")
    print("=" * 60)
    
    if integration_success:
        print("âœ… Integration tests: PASSED")
    else:
        print("âŒ Integration tests: FAILED")
    
    if unit_success:
        print("âœ… Unit tests: PASSED")
    else:
        print("âŒ Unit tests: FAILED")
    
    if summary.get("coverage_analysis"):
        coverage = summary["coverage_analysis"]["total_coverage"]
        grade = summary["coverage_analysis"]["grade"]
        print(f"ðŸ“ˆ Code coverage: {coverage:.2f}% ({grade})")
    
    print(f"\nðŸ“ Reports available:")
    print(f"   - HTML: coverage_html_report/index.html")
    print(f"   - JSON: coverage.json")
    print(f"   - XML: coverage.xml")
    print(f"   - Summary: test_coverage_summary.json")
    
    # Return success if both test types passed and coverage is reasonable
    overall_success = (
        integration_success and 
        unit_success and 
        (summary.get("coverage_analysis", {}).get("total_coverage", 0) >= 50)
    )
    
    return overall_success

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
